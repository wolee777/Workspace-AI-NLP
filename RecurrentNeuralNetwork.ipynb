{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 순환 신경망(Recurrent Neural Network)\n",
    "\n",
    "- 피드 포워드 신경망은 입력의 길이가 고정되어 있어 자연어 처리를 위한 신경망으로는 한계가 있었습니다. \n",
    "- 결국 다양한 길이의 입력 시퀀스를 처리할 수 있는 인공 신경망이 필요하게 되었는데, 자연어 처리에 대표적으로 사용되는 인공 신경망인 RNN, LSTM 등에 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN(Recurrent Neural Network)은 시퀀스(Sequence) 모델입니다. \n",
    "- 입력과 출력을 시퀀스 단위로 처리하는 모델입니다. \n",
    "- 번역기를 생각해보면 입력은 번역하고자 하는 문장. 즉, 단어 시퀀스입니다. \n",
    "- 출력에 해당되는 번역된 문장 또한 단어 시퀀스입니다. 이러한 시퀀스들을 처리하기 위해 고안된 모델들을 시퀀스 모델이라고 합니다. \n",
    "- 그 중에서도 RNN은 딥 러닝에 있어 가장 기본적인 시퀀스 모델입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 순환 신경망(Recurrent Neural Network, RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 파이썬으로 RNN 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직접 Numpy로 RNN 층을 구현해보겠습니다. 앞서 메모리 셀에서 은닉 상태를 계산하는 식을 다음과 같이 정의하였습니다.\n",
    "\n",
    "ht = tanh( WxXt + Whht-1 + b )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "의사 코드(pseudocode)로 실제 동작하는 코드가 아님. \n",
    "\n",
    "hidden_state_t = 0 # 초기 은닉 상태를 0(벡터)로 초기화\n",
    "for input_t in input_length: # 각 시점마다 입력을 받는다.\n",
    "    output_t = tanh(input_t, hidden_state_t) # 각 시점에 대해서 입력과 은닉 상태를 가지고 연산\n",
    "    hidden_state_t = output_t # 계산 결과는 현재 시점의 은닉 상태가 된다.\n",
    "    \n",
    "- 우선 t 시점의 은닉 상태를 hidden_state_t라는 변수로 선언하였고, 입력 데이터의 길이를 input_length로 선언하였습니다. \n",
    "- 이 경우, 입력 데이터의 길이는 곧 총 시점의 수(timesteps)가 됩니다. \n",
    "- 그리고 t 시점의 입력값을 input_t로 선언하였습니다. 각 메모리 셀은 각 시점마다 input_t와 hidden_sate_t(이전 상태의 은닉 상태)를 입력으로 활성화 함수인 하이퍼볼릭탄젠트 함수를 통해 현 시점의 hidden_state_t를 계산합니다.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래의 코드는 이해를 돕기 위해 (timesteps, input_dim) 크기의 2D 텐서를 입력으로 받았다고 가정하였으나, \n",
    "- 실제로 케라스에서는 (batch_size, timesteps, input_dim)의 크기의 3D 텐서를 입력으로 받는 것을 기억합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 10 # 시점의 수. NLP에서는 보통 문장의 길이가 된다.\n",
    "input_dim = 4 # 입력의 차원. NLP에서는 보통 단어 벡터의 차원이 된다.\n",
    "hidden_size = 8 # 은닉 상태의 크기. 메모리 셀의 용량이다.\n",
    "\n",
    "inputs = np.random.random( ( timesteps, input_dim ) ) # 입력에 해당되는 2D 텐서\n",
    "\n",
    "hidden_state_t = np.zeros( ( hidden_size, ) ) # 초기 은닉 상태는 0(벡터)로 초기화\n",
    "# 은닉 상태의 크기 hidden_size로 은닉 상태를 만듬."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 시점, 입력의 차원, 은닉 상태의 크기, 그리고 초기 은닉 상태를 정의하였습니다. \n",
    "현재 초기 은닉 상태는 0의 값을 가지는 벡터로 초기화가 된 상태입니다. 초기 은닉 상태를 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(hidden_state_t) # 8의 크기를 가지는 은닉 상태. 현재는 초기 은닉 상태로 모든 차원이 0의 값을 가짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "은닉 상태의 크기를 8로 정의하였으므로 8의 차원을 가지는 0의 값으로 구성된 벡터가 출력됩니다. \n",
    "\n",
    "이제 가중치와 편향을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx = np.random.random( ( hidden_size, input_dim ) )  # (8, 4)크기의 2D 텐서 생성. 입력에 대한 가중치.\n",
    "Wh = np.random.random( ( hidden_size, hidden_size ) ) # (8, 8)크기의 2D 텐서 생성. 은닉 상태에 대한 가중치.\n",
    "b = np.random.random( ( hidden_size, ) ) # (8,)크기의 1D 텐서 생성. 이 값은 편향(bias)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치와 편향을 각 크기에 맞게 정의하였습니다. \n",
    "\n",
    "가중치와 편향의 크기를 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4)\n",
      "(8, 8)\n",
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "print( np.shape( Wx ) )\n",
    "print( np.shape( Wh ) )\n",
    "print( np.shape( b ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 가중치와 편향의 크기는 다음과 같습니다. Wx는 (은닉 상태의 크기 × 입력의 차원), Wh는 (은닉 상태의 크기 × 은닉 상태의 크기), b는 (은닉 상태의 크기)의 크기를 가집니다. \n",
    "\n",
    "이제 모든 시점의 은닉 상태를 출력한다고 가정하고, RNN 층을 동작시켜봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "(2, 8)\n",
      "(3, 8)\n",
      "(4, 8)\n",
      "(5, 8)\n",
      "(6, 8)\n",
      "(7, 8)\n",
      "(8, 8)\n",
      "(9, 8)\n",
      "(10, 8)\n",
      "[[0.84099332 0.95726798 0.92011412 0.82458604 0.90833185 0.81422795\n",
      "  0.95059802 0.75486892]\n",
      " [0.99997297 0.99972403 0.99991796 0.99986669 0.99979064 0.99969978\n",
      "  0.99998984 0.99997157]\n",
      " [0.99999071 0.99975621 0.99998036 0.99989999 0.99986885 0.99981472\n",
      "  0.99999643 0.99996906]\n",
      " [0.99999121 0.99976524 0.99998046 0.99989764 0.99987047 0.99982974\n",
      "  0.99999671 0.99996905]\n",
      " [0.99999237 0.99992699 0.99997585 0.99996562 0.99991601 0.99991442\n",
      "  0.9999982  0.99999453]\n",
      " [0.99999602 0.9999548  0.99998382 0.99997371 0.99993849 0.99995978\n",
      "  0.99999918 0.9999956 ]\n",
      " [0.99999787 0.99997121 0.99999249 0.99999097 0.99996828 0.99997207\n",
      "  0.99999949 0.99999803]\n",
      " [0.99998818 0.99991497 0.99996244 0.9999545  0.99989027 0.9998837\n",
      "  0.99999738 0.99999413]\n",
      " [0.99998785 0.99991632 0.99996636 0.99994536 0.9998595  0.99988284\n",
      "  0.99999756 0.99999014]\n",
      " [0.99997474 0.99979907 0.99993322 0.99990616 0.99983143 0.99970575\n",
      "  0.99999277 0.99998955]]\n"
     ]
    }
   ],
   "source": [
    "total_hidden_states = []\n",
    "\n",
    "# 메모리 셀 동작\n",
    "for input_t in inputs: # 각 시점에 따라서 입력값이 입력됨.\n",
    "  output_t = np.tanh( np.dot( Wx,input_t ) + np.dot( Wh,hidden_state_t ) + b ) # Wx * Xt + Wh * Ht-1 + b(bias)\n",
    "  total_hidden_states.append( list( output_t ) ) # 각 시점의 은닉 상태의 값을 계속해서 축적\n",
    "  print( np.shape( total_hidden_states ) ) # 각 시점 t별 메모리 셀의 출력의 크기는 (timestep, output_dim)\n",
    "  hidden_state_t = output_t\n",
    "\n",
    "total_hidden_states = np.stack( total_hidden_states, axis = 0 ) \n",
    "# 출력 시 값을 깔끔하게 해준다.\n",
    "\n",
    "print( total_hidden_states ) # (timesteps, output_dim)의 크기. 이 경우 (10, 8)의 크기를 가지는 메모리 셀의 2D 텐서를 출력."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 케라스(Keras)로 RNN 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "케라스로 RNN 층을 추가하는 코드는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN 층을 추가하는 코드.\n",
    "model.add( SimpleRNN( hidden_size ) ) # 가장 간단한 형태"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인자를 사용할 때를 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가 인자를 사용할 때\n",
    "model.add( SimpleRNN( hidden_size, input_shape = ( timesteps, input_dim ) ) )\n",
    "\n",
    "# 다른 표기\n",
    "model.add( SimpleRNN( hidden_size, input_length = M, input_dim = N ) )\n",
    "# 단, M과 N은 정수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden_size = 은닉 상태의 크기를 정의. 메모리 셀이 다음 시점의 메모리 셀과 출력층으로 보내는 값의 크기(output_dim)와도 동일. RNN의 용량(capacity)을 늘린다고 보면 되며, 중소형 모델의 경우 보통 128, 256, 512, 1024 등의 값을 가진다.\n",
    "- timesteps = 입력 시퀀스의 길이(input_length)라고 표현하기도 함. 시점의 수.\n",
    "- input_dim = 입력의 크기."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text]( rnn_image6between7.png )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN 층은 (batch_size, timesteps, input_dim) 크기의 3D 텐서를 입력으로 받습니다. \n",
    "- batch_size는 한 번에 학습하는 데이터의 개수를 말합니다. (여기서부터는 텐서의 개념을 반드시 이해해야 하므로 벡터와 행렬 연산 챕터의 텐서 설명 부분을 참고하시기 바랍니다.) \n",
    "- 다만, 이러한 표현은 사람이나 문헌에 따라서, 또는 풀고자 하는 문제에 따라서 종종 다르게 기재되는데 위의 그림은 문제와 상황에 따라서 다르게 표현되는 입력 3D 텐서의 대표적인 표현들을 보여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 헷갈리지 말아야할 점은 위의 코드는 출력층까지 포함한 하나의 완성된 인공 신경망 코드가 아니라 은닉층. \n",
    "- 즉, RNN 층에 대한 코드라는 점입니다. \n",
    "- 해당 코드가 리턴하는 결과값은 출력층의 결과가 아니라 하나의 은닉 상태 또는 정의하기에 따라 다수의 은닉 상태 입니다. \n",
    "- 아래의 그림은 앞서 배운 출력층을 포함한 완성된 인공 신경망 그림과 은닉층까지만 표현한 그림의 차이를 보여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text]( rnn_image7_ver2.png )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그렇다면 RNN 층은 위에서 설명한 입력 3D 텐서를 입력받아서 어떻게 은닉 상태를 출력할까요? \n",
    "- RNN 층은 사용자의 설정에 따라 두 가지 종류의 출력을 내보냅니다. \n",
    "- 메모리 셀의 최종 시점의 은닉 상태만을 리턴하고자 한다면 (batch_size, output_dim) 크기의 2D 텐서를 리턴합니다. \n",
    "- 하지만, 메모리 셀의 각 시점(time step)의 은닉 상태값들을 모아서 전체 시퀀스를 리턴하고자 한다면 (batch_size, timesteps, output_dim) 크기의 3D 텐서를 리턴합니다. \n",
    "- 이는 RNN 층의 return_sequences 매개 변수에 True를 설정하여 설정이 가능합니다. (output_dim은 앞서 코드에서 정의한 hidden_size의 값으로 설정됩니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text]( rnn_image8_ver2.png )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림은 time step=3일 때, return_sequences = True를 설정했을 때와 그렇지 않았을 때 어떤 차이가 있는지를 보여줍니다.\n",
    "- return_sequences=True를 선택하면 메모리 셀이 모든 시점(time step)에 대해서 은닉 상태값을 출력하며, 별도 기재하지 않거나 return_sequences=False로 선택할 경우에는 메모리 셀은 하나의 은닉 상태값만을 출력합니다. \n",
    "- 그리고 이 하나의 값은 마지막 시점(time step)의 메모리 셀의 은닉 상태값입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 마지막 은닉 상태만 전달하도록 하면 many-to-one 문제를 풀 수 있고, 모든 시점의 은닉 상태를 전달하도록 하면, \n",
    "- 다음층에 은닉층이 하나 더 있는 경우이거나 many-to-many 문제를 풀 수 있습니다.\n",
    "   \n",
    "- LSTM이나 GRU도 내부 메커니즘은 다르지만 model.add()를 통해서 층을 추가하는 코드는 사실상 SimpleRNN 코드와 같은 형태를 가집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력값이 (batch_size, output_dim) 크기의 2D 텐서일 때, output_dim은 hidden_size의 값인 3입니다. 이 경우 batch_size를 현 단계에서는 알 수 없으므로 (None, 3)이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn (SimpleRNN)       (None, 3)                 42        \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add( SimpleRNN( 3, input_shape = ( 2,10 ) ) )\n",
    "# model.add(SimpleRNN(3, input_length=2, input_dim=10))와 동일함.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size를 8로 기재하자, 출력의 크기가 (8, 3)이 된 것을 볼 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (8, 3)                    42        \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add( SimpleRNN( 3, batch_input_shape = ( 8, 2, 10 ) ) )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- return_sequences 매개 변수에 True를 기재하여 출력값으로 (batch_size, timesteps, output_dim) 크기의 3D 텐서를 리턴하도록 모델\n",
    "- 출력의 크기가 (8, 2, 3)이 된 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_2 (SimpleRNN)     (8, 2, 3)                 42        \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add( SimpleRNN( 3, batch_input_shape = ( 8, 2, 10 ), return_sequences = True ) )\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
