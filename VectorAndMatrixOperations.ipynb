{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 벡터와 행렬 연산\n",
    "\n",
    "- NumPy나 Tensorflow의 low-level의 머신 러닝 개발을 하게되면 각 변수들의 연산을 벡터와 행렬 연산으로 이해할 수 있어야 한다.\n",
    "- 사용자가 변수의 개수로부터 행렬의 크기, 더 나아 텐서의 크기를 산정할 수 있어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 벡터와 행렬과 텐서\n",
    "\n",
    "- 벡터는 크기와 방향을 가진 양이다. 숫자가 나열된 형상이며 파이썬에서는 1차원 배열 또는 리스트로 표현\n",
    "- 행렬은 행과 열을 가진 2차원 형상을 가진 구조이다. 파이썬에서는 2차원 배열로 표현, 가로줄은 행( row ), 세로줄은 열( column )\n",
    "- 3차원 부터는 텐서라 부르고 파이썬에서는 3차원 이상의 배열로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 텐서( Tensor )\n",
    "\n",
    "- 인공 신경망은 복잡한 모델 내의 연산을 주로 행렬 연산을 통해 해결한다. \n",
    "- 여기서 말하는 행렬 연산은 2차원 배열을 통한 행렬 연산만을 의미하는 것은 아니다.\n",
    "- 머신 러닝의 입/출력이 복잡해지면 3차원 텐서에 대한 이해가 필수로 요구된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 0차원 텐서( 0D 텐서, 차원 : Dimensionality )\n",
    "\n",
    "- 스칼라는 하나의 실수값으로 이루어진 데이터를 말한다. 또한 스칼라값을 0차원 텐서라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "d = np.array( 5 )\n",
    "\n",
    "print( d.ndim ) # 축의 개수, 텐서의 차원수와 동일\n",
    "print( d.shape ) # 텐서 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 1차원 텐서( 1D 텐서 )\n",
    "\n",
    "- 숫자를 특정 순서대로 배열한 것을 벡터라고 한다. 벡터를 1차원 텐서라고 한다.\n",
    "- 벡터의 차원과 텐서의 차원은 다른 개념이다.\n",
    "- 벡터의 차원과 텐서의 차원의 정의로 인해 혼동할 수 있으므로 벡터에서의 차원( Dimensionality )은 하나의 축에 차원들이 존재하는 것이고, 텐서에서의 차원( Dimensionality )은 축의 개수를 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "# 4차원 벡터이지만 1차원 텐서이다.\n",
    "d = np.array( [ 1, 2, 3, 4 ] )\n",
    "\n",
    "print( d.ndim )\n",
    "print( d.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 2차원 텐서( 2D 텐서 )\n",
    "\n",
    "- 행과 열이 존재하는 벡터의 배열. 즉, 행렬( matrix )을 2차원 텐서라 한다.\n",
    "- 텐서의 크기를 미리 머리 속에 그릴 수 있으면 모델 설계 시에 유용하다.\n",
    "- 1차원 텐서를 벡터, 2차원 텐서를 행렬로 비유하는데 수학적으로 행렬의 열을 열벡터로 부르거나, 열벡터를 열행렬로 부르는 것과 혼동해서는 안된다.\n",
    "- 1차원 텐서와 2차원 텐서는 차원 자체가 달라야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "d = np.array( [ [ 1, 2, 3, 4 ], [ 5, 6, 7, 8 ], [ 9, 10, 11, 12 ] ] )\n",
    "\n",
    "print( d.ndim )\n",
    "print( d.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 3차원 텐서( 3D 텐서 )\n",
    "\n",
    "- 행렬 또는 2차원 텐서를 단위로 한 번 더 배열하면 3차원 텐서라 부른다.\n",
    "- 0차원 ~ 2차원 텐서는 각각 스칼라, 벡터, 행렬로 해도 무방한다.\n",
    "- 3차원 이상의 텐서부터 본격적으로 텐서라고 부른다.\n",
    "- 데이터 사이언스 분야 한정으로 주로 3차원 이상의 배열을 텐서라고 부른다.\n",
    "- 3차원 텐서의 구조를 이해하지 않으면, 복잡한 인공 신경망의 입/출력값을 이해하는 것이 쉽지 않다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(2, 3, 5)\n"
     ]
    }
   ],
   "source": [
    "# 2개의 큰 데이터가 있는데, 그 각각은 3개의 더 작은 데이터로 구성되며, 그 3개의 데이터는 또한 더 작은 5개의 데이터로 구성\n",
    "d = np.array( [ \n",
    "                [ [ 1, 2, 3, 4, 5 ], [ 6, 7, 8, 9, 10 ], [ 11, 12, 13, 14, 15 ] ],\n",
    "                [ [ 16, 17, 18, 19, 20 ], [ 21, 22, 23, 24, 25 ], [ 26, 27, 28, 29, 30 ] ]\n",
    "              ] )\n",
    "\n",
    "print( d.ndim )\n",
    "print( d.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 자연어 처리에서 특히 자주 보게 되는것이 3D 텐서이다.\n",
    "- 3D 텐서는 시퀀스 데이터( sequence data )를 표현할 때 자주 사용\n",
    "- 시퀀스 데이터는 주로 단어의 시퀀스를 의미하며, 시퀀스는 주로 문장이나 문서, 뉴스 기사 등의 텍스트가 될 수 있다.\n",
    "- 3D 텐서는 ( samples, timesteps, word_dim ), 일괄로 처리하기 위해 데이터를 묶는 단위는 배치의 개념의( batch_size, timesteps, word_dim ) 이라 볼수 있아.\n",
    "- samples/batch_size는 데이터의 개수, timesteps는 시퀀스의 길이, word_dim은 단어를 표현하는 벡터의 차원을 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 자연어 처리에서 왜 3D 텐서의 개념이 사용되는지 간단한 예\n",
    "\n",
    "훈련 데이터\n",
    "\n",
    "- 문서1 : I like NLP\n",
    "- 문서2 : I like DL\n",
    "- 문서3 : DL is AI\n",
    "\n",
    "이를 인공 신경망의 모델의 입력으로 사용하기 위해서는 각 단어를 벡터화해야 한다. 단어를 벡터화 방법인 원-핫 인코딩으로 벡터화 하면\n",
    "\n",
    "|단어|One-hot vector|\n",
    "|---|---|\n",
    "I|[ 1 0 0 0 0 0 ]\n",
    "like|[ 0 1 0 0 0 0 ]\n",
    "NLP|[ 0 0 1 0 0 0 ]\n",
    "DL|[ 0 0 0 1 0 0 ]\n",
    "is|[ 0 0 0 0 1 0 ]\n",
    "AI|[ 0 0 0 0 0 1 ]\n",
    "\n",
    "기존에 있던 훈련 데이터를 모두 원-핫 벡터로 바꿔서 인공 신경망의 입력으로 한꺼번에 사용한다고 하면 다음과 같다.( 이렇게 훈련 데이터를 여러개 묶어서 한꺼번에 입력으로 사용하는 것을 배치( batch )라고 한다.\n",
    "\n",
    "[ \n",
    "\n",
    "  [ [ 1, 0, 0, 0, 0, 0 ], [ 0, 1, 0, 0, 0, 0 ], [ 0, 0, 1, 0, 0, 0 ] ],\n",
    "  \n",
    "  [ [ 1, 0, 0, 0, 0, 0 ], [ 0, 1, 0, 0, 0, 0 ], [ 0, 0, 0, 1, 0, 0 ] ],\n",
    "  \n",
    "  [ [ 0, 0, 0, 1, 0, 0 ], [ 0, 0, 0, 0, 1, 0 ], [ 0, 0, 0, 0, 0, 1 ] ]\n",
    "  \n",
    "]\n",
    "\n",
    "이는 ( 3, 3, 6 )의 크기를 가지는 3D 텐서이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) 그 이상의 텐서\n",
    "\n",
    "- 3차원 텐서를 배열로 합치면 4차원 텐서가 된다. 4차원 텐서를 배열로 합치면 5차원 텐서가 된다.\n",
    "- 이런 식으로 텐서는 배열로서 계속해서 확장될 수 있다.\n",
    "\n",
    "### 6) 케라스에서의 텐서\n",
    "\n",
    "- NumPy로 각 텐서의 ndim( 차원 )과 shape( 크기 )를 사용했다. 3차원 텐서는 3차원이고 크기는 ( 2, 3, 5 )와 같이 표현\n",
    "- 케라스에서는 입력의 크기( shape )를 인자로 줄 때 input_shape라는 인자를 사용한다.\n",
    "- input_shape는 배치 크기를 제외하고 차원을 지정, 예를 들어 input_shape( 6, 5 )라는 인자값을 사용하고 배치 크기를 32라고 가정하면 텐서의 크기는 ( 32, 6, 5 )를 의미. 만약 배치 크기까지 지정하고 싶으면 batch_input_shape = ( 8, 2, 10 )와 같이 인자를 주면 텐서의 크기는 ( 8, 2, 10 )을 의미\n",
    "- 입력의 속성 수를 의미하는 input_dim, 시퀀스 데이터의 길이를 의미하는 input_length등의 인자도 사용\n",
    "- input_shape의 두 개의 인자는 ( input_length, input_dim )라고 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 벡터와 행렬의 연산\n",
    "\n",
    "### 1) 벡터와 행령의 덧셈과 뺄셈\n",
    "\n",
    "- 같은 크기의 두 개의 벡터나 행렬은 덧셈과 뺄샘을 할 수 있다. 이 경우 같은 위치의 원소끼리 연산하면 된다.\n",
    "- 이러한 연산은 요소별( element_wise )연산이라 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 6 8]\n",
      "\n",
      "[7 2 2]\n"
     ]
    }
   ],
   "source": [
    "# 두 벡터간 덧셈, 뺄셈\n",
    "a = np.array( [ 8, 4, 5 ] )\n",
    "b = np.array( [ 1, 2, 3 ] )\n",
    "\n",
    "print( a + b )\n",
    "print()\n",
    "print( a - b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15 26 37 48]\n",
      " [51 62 73 84]]\n",
      "\n",
      "[[ 5 14 23 32]\n",
      " [49 58 67 76]]\n"
     ]
    }
   ],
   "source": [
    "# 두 행렬간 덧셈, 뺄셈\n",
    "a = np.array( [ [ 10, 20, 30, 40 ], [ 50, 60, 70, 80 ] ] )\n",
    "b = np.array( [ [ 5, 6, 7, 8 ], [ 1, 2, 3, 4 ] ] )\n",
    "\n",
    "print( a + b )\n",
    "print()\n",
    "print( a - b )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 벡터의 내적과 행렬의 곱셈\n",
    "\n",
    "- 벡터의 접곱( dot product ) 또는 내적( inner product )\n",
    "- 벡터의 내적은 연산을 점( dot )으로 표현하며 a . b와 같이 표현하기도 한다.\n",
    "- 내적이 성립하기 위해서는 두 벡터의 차원이 같아야 하며, 두 벡터 중 앞의 벡터가 행벡텨( 가로 방향 벡터 )이고 뒤의 벡터가 열벡터( 세로 방향 벡터 )여야 한다.\n",
    "- 벡터의 내적의 결과는 스칼라가 된다는 특징이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "a = np.array( [ 1, 2, 3 ] )\n",
    "b = np.array( [ 4, 5, 6 ] )\n",
    "\n",
    "print( np.dot( a, b ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 행렬의 곱셈을 이해하기 위해서는 벡터의 내적을 이해해야 한다.\n",
    "- 행렬의 곱셈은 왼쪽 행렬의 행벡터( 가로 방향 벡터 )와 오른쪽 행렬의 열벡터( 세로 방향 벡터 )의 내적( 대응하는 원소들의 곱의 합 )이 결과 행렬의 원소가 되는 것으로 이루어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23 31]\n",
      " [34 46]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array( [ [ 1, 3 ], [ 2, 4 ] ] )\n",
    "b = np.array( [ [ 5, 7 ], [ 6, 8 ] ] )\n",
    "\n",
    "print( np.matmul( a, b ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 벡터와 행렬의 곱 또는 행렬과 벡터의 곱 또한 행렬의 곱셈과 동일한 원리로 이루어 진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 다중 선형 회귀 행렬 연산으로 이해하기\n",
    "\n",
    "- 독립 변수가 2개 이상일 때, 1개의 종속 변수를 예측하는 문제를 행렬의 연산으로 표현한다면 어떻게 될까?\n",
    "- 다중 선형 회귀나 다중 로지스틱 회귀가 이러한 연산의 예이다.\n",
    "- 다중 선형 회귀를 예를 들면, 독립 변수 x가 n개인 다중 선형 회귀 수식\n",
    "\n",
    "y = w1x1 + w2x2 + w3x3 + ... + wnxn + b\n",
    "\n",
    "- 이는 입력 벡터 [ x1, ... , xn ]와 가중치 벡터 [ w1, ... , wn ]의 내적으로 표현할 수 있다.\n",
    "- 또한 가중치 벡터 [ w1, ... , wn ]와 입력 벡터 [ x1, ... , xn ]의 내적으로 표현할 수도 있다.\n",
    "\n",
    "- 그런데 데이터의 개수가 많을 경우에는 벡터의 내적이 아니라 행렬의 곱셈으로 표현이 가능하다.\n",
    "\n",
    "- 다중 선형 회귀에서 데이터의 개수가 여러개일 때, 행렬의 곱셈으로 어떻게 표현할 수 있는지 예를 들어보면 다음과 같다.\n",
    "- 다음은 집의 크기, 방의 수, 층의 수, 집이 얼마나 오래되었는지와 집의 가격이 기록된 부동산 데이터라고 가정하고 해당 데이터를 학습하여 새로운 집의 정보가 들어왔을 때, 집의 가격을 예측해본다고 하자.\n",
    "\n",
    "|size( feet2 )( x1 )|number of bedrooms( x2 )|number of floors( x3 )|age of home( x4 )|price( $1000 )( y )|\n",
    "|---|---|---|---|---|\n",
    "|1800|2|1|10|207|\n",
    "|1200|4|2|20|176|\n",
    "|1700|3|2|15|213|\n",
    "|1500|5|1|10|234|\n",
    "|1100|2|2|10|155|\n",
    "\n",
    "- 위 데이터에 대해 입력 행렬  X와 가중치 벡터 W의 곱으로 표현하면 다음과 같다.\n",
    "\n",
    "x11 x12 x13 x14    w1      x11 w1 + x12 w2 + x13 w3 + x14 w4\n",
    "\n",
    "x21 x22 x23 x24    w2      x21 w1 + x22 w2 + x23 w3 + x24 w4\n",
    "\n",
    "x31 x32 x33 x34    w3   =  x31 w1 + x32 w2 + x33 w3 + x34 w4\n",
    "\n",
    "x41 x42 x43 x44    w4      x41 w1 + x42 w2 + x43 w3 + x44 w4\n",
    "\n",
    "x51 x52 x53 x54            x51 w1 + x52 w2 + x53 w3 + x54 w4\n",
    "\n",
    "- 여기에 편향 B를 더 해주면 위 데이터에 대한 전체 가설 수식 H( X )를 표현할 수 있다.\n",
    "\n",
    "x11 w1 + x12 w2 + x13 w3 + x14 w4     b    y1\n",
    "\n",
    "x21 w1 + x22 w2 + x23 w3 + x24 w4     b    y2\n",
    "\n",
    "x31 w1 + x32 w2 + x33 w3 + x34 w4  +  b  = y3 \n",
    "\n",
    "x41 w1 + x42 w2 + x43 w3 + x44 w4     b    y4\n",
    "\n",
    "x51 w1 + x52 w2 + x53 w3 + x54 w4     b    y5\n",
    "\n",
    "H( X ) = XW + B\n",
    "\n",
    "- 위의 수식에서 입력 행렬 X는 5행 4열의 크기를 가진다. 출력 벡터를 Y라고 하였을 때 Y는 5행 1열의 크기를 가집니다. 여기서 곱셈이 성립하기 위해서 가중치 벡터 W의 크기는 4행 1열을 가져야함을 추론할 수 있습니다.\n",
    "\n",
    "- 만약 가중치 벡터를 앞에 두고 입력 행렬을 뒤에 두고 행렬 연산을 한다면 이는 아래와 같습니다.\n",
    "\n",
    "                 x11 w1 + x12 w2 + x13 w3 + x14 w4\n",
    "                 \n",
    "                 x21 w1 + x22 w2 + x23 w3 + x24 w4     \n",
    "                 \n",
    "[ w1 w2 w3 w4 ]   x31 w1 + x32 w2 + x33 w3 + x34 w4  +  [ b b b b b ]  = [ y1 y2 y3 y4 y5 ]\n",
    "                 \n",
    "                 x41 w1 + x42 w2 + x43 w3 + x44 w4     \n",
    "                 \n",
    "                 x51 w1 + x52 w2 + x53 w3 + x54 w4     \n",
    "                 \n",
    "- 이는 연산 방법의 차이일뿐 어떤 것이 맞다고, 틀리다고 할 수는 없습니다. 다만, 수학적 관례로 아래와 같이 수식으로 표현할 때는 주로 가중치 W가 입력 X의 앞에 오므로 위 두 가지 경우를 모두 염두하는 것이 좋습니다.       \n",
    "\n",
    "H( x ) = WX + B\n",
    "\n",
    "- 인공 신경망도 본질적으로 위와 같이 훈련 데이터에 대한 행렬 연산이므로 이번 예제는 반드시 이해해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 샘플( Sample )과 특성( Feature )\n",
    "\n",
    "- 훈련 데이터의 입력 행렬을 X라고 하였을 때 샘플(Sample)과 특성(Feature)의 정의는 다음과 같습니다.\n",
    "\n",
    "![Alt text]( n_x_m.png )\n",
    "\n",
    "- 머신 러닝에서는 데이터를 셀 수 있는 단위로 구분할 때, 각각을 샘플이라고 부르며, 종속 변수 y를 예측하기 위한 각각의 독립 변수 x를 특성이라고 부릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 가중치와 편향 행렬의 크기 결정\n",
    "\n",
    "- 여기서는 특성을 행렬의 열로 보는 경우를 가정하여 행렬의 크기가 어떻게 결정되는지 정리합니다.\n",
    "\n",
    "- 행렬곱은 두 가지 정의를 가지는데, 두 개의 행렬 J와 K의 곱은 다음과 같은 조건을 충족해야 합니다.\n",
    "1) 두 행렬의 곱 J x K에 대하여 행렬 J의 열의 수와 행렬 K의 행의 수는 같아야 한다.\n",
    "2) 두 행렬의 곱 J x K의 결과로 나온 행렬 JK의 크기는 J의 행의 크기와 K의 열의 크기를 가진다.\n",
    "\n",
    "- 이로부터 주어진 데이터가 입력과 출력의 행렬의 크기를 어떻게 가지느냐에 따라서 가중치 W의 행렬과 편향 b의 행렬의 크기를 찾아낼 수 있습니다. 독립 변수 x의 행렬을 X, 종속 변수 y의 행렬을 Y라고 하였을 때, 가중치 W의 행렬을 W, 편향 b의 행렬을 B라고 해봅시다. 이때 행렬 X는 또한 입력 행렬(Input Matrix)이라고 부를 수 있고, Y는 출력 행렬(Output Matrix)이라고 부를 수 있습니다.\n",
    "\n",
    "![Alt text]( matrix1.png )\n",
    "\n",
    "- 이제 입력 행렬의 크기와 출력 행렬의 크기로부터 W행렬과 B행렬의 크기를 추론해봅시다.\n",
    "\n",
    "![Alt text]( matrix2.png )\n",
    "\n",
    "- 우선 행렬의 덧셈에 해당되는 B행렬은 Y행렬의 크기에 영향을 주지 않습니다. 그러므로 B행렬의 크기는 Y행렬의 크기와 같습니다.\n",
    "\n",
    "![Alt text]( matrix3.png )\n",
    "\n",
    "- 행렬의 곱셈이 성립되려면 행렬의 곱셈에서 앞에 있는 행렬의 열의 크기와 뒤에 있는 행렬의 행의 크기는 같아야 합니다. 그러므로 입력 행렬 X로부터 W행렬의 행의 크기가 결정됩니다.\n",
    "\n",
    "![Alt text]( matrix4.png )\n",
    "\n",
    "- 두 행렬의 곱의 결과로서 나온 행렬의 열의 크기는 행렬의 곱에서 뒤에 있는 행렬의 열의 크기와 동일합니다. 그러므로 출력 행렬 Y로부터 W행렬의 열의 크기가 결정됩니다. 이 때 위의 식에서 X행렬의 행을 의미하는 수치 m은 샘플 데이터를 몇 개씩 묶어서 처리하느냐에 따라 달라집니다. 전체 샘플 데이터에 대해서 한 꺼번에 행렬 연산을 하고자한다면, m은 전체 샘플의 개수가 됩니다. 위에서 본 다중 선형 회귀에서 5개의 전체 샘플을 가지고 세웠던 행렬 연산식이 그러한 예입니다.\n",
    "\n",
    "- 하지만 전체 샘플 데이터 중 1개씩 불러와서 처리하고자한다면 m은 1이 됩니다. 또는 전체 데이터를 임의의 m개씩 묶인 작은 그룹들로 분할하여 여러번 처리할 수도 있는데 이렇게 처리하면서 기계가 학습하는 것을 ***미니배치 학습***이라고 합니다. 예를 들어 전체 데이터가 1,024개가 있을 때 m을 64로 잡는다면 전체 데이터는 16개의 그룹으로 분할됩니다. 각 그룹은 총 64개의 샘플로 구성됩니다. 그리고 위에서 설명한 행렬 연산을 총 16번 반복하게되고 그제서야 전체 데이터에 대한 학습이 완료됩니다. 이때 64를 ***배치 크기(Batch size)***라고 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
